# -*- coding: utf-8 -*-
"""emotion_detect.ipynb

Automatically generated by Colaboratory.



# !pip install deepface
# installing the deepface library

"""# reading the video file and detedting face and then classifying the emotion"""

# face emotion detection in live camera
import cv2
# importing the deepface library
from deepface import DeepFace

# from google.colab.patches import cv2_imshow

# download the file 'haarcascade_frontalface_default.xml'

face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml") #Note the change
# reading the video,its width and height for frames
cap = cv2.VideoCapture('/content/emotion.mp4')
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))

# writing the output video
out = cv2.VideoWriter('outpy2.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))
# a loop to capture each frame and perform actions on it
while True:

    ret,frame = cap.read()
    # storing the emotion in result
    result = DeepFace.analyze(img_path = frame , actions=['emotion'], enforce_detection=False )
    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray,1.1,4)
    # creating a rectangle(blue color) around the face 
    for (x,y,w,h) in faces:
        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),3)
    #  getting the dominant emotion
    emotion = result["dominant_emotion"]
    # converting the dominant emotion to string format
    txt = str(emotion)
    # putting the emotion  name on the frame
    cv2.putText(frame,txt,(50,50),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)
    # showing the each frame
    cv2.imshow(frame)
    # adding the frame to output file
    out.write(frame)

    if cv2.waitKey(1) & 0xff == ord('q'):
        break
# releasing software resorse
cap.release()
out.release()
# destroying all videos
cv2.destroyAllWindows()

